{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cd5cc78",
   "metadata": {},
   "source": [
    "## AlexNet from scratch model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f88d52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 8, 8, 96)          34944     \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 8, 8, 96)          384       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " activation (Activation)     (None, 8, 8, 96)          0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 4, 4, 96)          0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 4, 4, 256)         614656    \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 4, 4, 256)         1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 2, 2, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 2, 2, 384)         885120    \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 2, 2, 384)         1536      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 2, 2, 384)         0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 2, 2, 384)         1327488   \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 2, 2, 384)         1536      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 2, 2, 384)         0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 2, 2, 256)         884992    \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 2, 2, 256)         1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 2, 2, 256)         0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 1, 1, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              1052672   \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 4096)              16384     \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 4096)              0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 4096)              16384     \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 4096)              0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1000)              4097000   \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 1000)              4000      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 1000)              0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                10010     \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 10)                40        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25730506 (98.15 MB)\n",
      "Trainable params: 25709350 (98.07 MB)\n",
      "Non-trainable params: 21156 (82.64 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "import keras \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.layers import BatchNormalization\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(1000)\n",
    "\n",
    "\n",
    "# Instantiation\n",
    "AlexNet = Sequential()\n",
    "\n",
    "# 1st Convolutional Layer\n",
    "AlexNet.add(Conv2D(filters=96, input_shape=(32,32,3), kernel_size=(11,11),strides=(4,4),padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "AlexNet.add(MaxPool2D(pool_size=(2,2), strides=(2,2),padding='same'))\n",
    "\n",
    "# 2nd Convolutional Layer\n",
    "AlexNet.add(Conv2D(filters=256, kernel_size=(5,5),strides=(1,1),padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "AlexNet.add(MaxPool2D(pool_size=(2,2),padding='same'))\n",
    "\n",
    "# 3rd Convolutional Layer\n",
    "AlexNet.add(Conv2D(filters=384, kernel_size=(3,3),strides=(1,1),padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "\n",
    "# 4th Convolutional Layer\n",
    "AlexNet.add(Conv2D(filters=384, kernel_size=(3,3),strides=(1,1),padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "\n",
    "# 5th Convolutional Layer\n",
    "AlexNet.add(Conv2D(filters=256, kernel_size=(3,3),strides=(1,1),padding='same'))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "AlexNet.add(MaxPool2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "\n",
    "\n",
    "# Passing it to a Fully Connected layer\n",
    "AlexNet.add(Flatten())\n",
    "\n",
    "# 1st Fully connected Layer \n",
    "AlexNet.add(Dense(4096, input_shape=(32,32,3,)))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "# Add Dropout to prevent Overfitting \n",
    "AlexNet.add(Dropout(0.4))\n",
    "\n",
    "# 2nd Fully connected Layer \n",
    "AlexNet.add(Dense(4096))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "# Add Dropout\n",
    "AlexNet.add(Dropout(0.4))\n",
    "\n",
    "# 3rd Fully connected Layer \n",
    "AlexNet.add(Dense(1000))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('relu'))\n",
    "# Add Dropout\n",
    "AlexNet.add(Dropout(0.4))\n",
    "\n",
    "# Output Layer\n",
    "AlexNet.add(Dense(10))\n",
    "AlexNet.add(BatchNormalization())\n",
    "AlexNet.add(Activation('softmax'))\n",
    "\n",
    "# Model Summary \n",
    "AlexNet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90eaf764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model \n",
    "AlexNet.compile(loss = keras.losses.categorical_crossentropy, optimizer= 'adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a2fd26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((35000, 32, 32, 3), (35000, 1))\n",
      "((15000, 32, 32, 3), (15000, 1))\n",
      "((10000, 32, 32, 3), (10000, 1))\n"
     ]
    }
   ],
   "source": [
    "# Keras Library for CIFAR dataset\n",
    "from keras.datasets import cifar10\n",
    "(x_train, y_train),(x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Train-Validation-Test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train,y_train, test_size =0.3)\n",
    "\n",
    "# Dimension of the CIFAR10 dataset \n",
    "print((x_train.shape, y_train.shape))\n",
    "print((x_val.shape, y_val.shape))\n",
    "print((x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cbfee6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((35000, 32, 32, 3), (35000, 10))\n",
      "((15000, 32, 32, 3), (15000, 10))\n",
      "((10000, 32, 32, 3), (10000, 10))\n"
     ]
    }
   ],
   "source": [
    "# One-hot Encoding the Labels \n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Since we have 10 calsses we should expect the shape[1] of y_train, y_val and y_test to change from 1 to 10 \n",
    "y_train = to_categorical(y_train)\n",
    "y_val = to_categorical(y_val)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "\n",
    "# Verifying the dimension after one-hot encoding \n",
    "print((x_train.shape, y_train.shape))\n",
    "print((x_val.shape, y_val.shape))\n",
    "print((x_test.shape, y_test.shape))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22f0fe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Data Augmentation\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_generator = ImageDataGenerator(rotation_range=2, horizontal_flip=True, zoom_range=0.1)\n",
    "val_generator = ImageDataGenerator(rotation_range=2, horizontal_flip=True, zoom_range=0.1)\n",
    "test_generator = ImageDataGenerator(rotation_range=2, horizontal_flip=True, zoom_range=0.1)\n",
    "\n",
    "# Fitting the augmentation defined above to the data\n",
    "train_generator.fit(x_train)\n",
    "val_generator.fit(x_val)\n",
    "test_generator.fit(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d12b17c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate Annealer\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "lrr = ReduceLROnPlateau( monitor='val_acc', factor= 0.01, patience= 3, min_lr= 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d5e90b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the parameters\n",
    "batch_size = 100\n",
    "epochs = 10\n",
    "learning_rate =0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2d14fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "350/350 [==============================] - ETA: 0s - loss: 1.6629 - accuracy: 0.4058WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "350/350 [==============================] - 561s 2s/step - loss: 1.6629 - accuracy: 0.4058 - val_loss: 2.2968 - val_accuracy: 0.1956 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "350/350 [==============================] - ETA: 0s - loss: 1.4088 - accuracy: 0.5031WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "350/350 [==============================] - 626s 2s/step - loss: 1.4088 - accuracy: 0.5031 - val_loss: 1.6885 - val_accuracy: 0.4138 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "350/350 [==============================] - ETA: 0s - loss: 1.2875 - accuracy: 0.5528WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "350/350 [==============================] - 551s 2s/step - loss: 1.2875 - accuracy: 0.5528 - val_loss: 2.1005 - val_accuracy: 0.2972 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "350/350 [==============================] - ETA: 0s - loss: 1.2016 - accuracy: 0.5819WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "350/350 [==============================] - 553s 2s/step - loss: 1.2016 - accuracy: 0.5819 - val_loss: 2.4567 - val_accuracy: 0.2349 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "350/350 [==============================] - ETA: 0s - loss: 1.1325 - accuracy: 0.6082WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "350/350 [==============================] - 551s 2s/step - loss: 1.1325 - accuracy: 0.6082 - val_loss: 1.5239 - val_accuracy: 0.4661 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "350/350 [==============================] - ETA: 0s - loss: 1.0628 - accuracy: 0.6332WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "350/350 [==============================] - 553s 2s/step - loss: 1.0628 - accuracy: 0.6332 - val_loss: 1.9500 - val_accuracy: 0.3836 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "350/350 [==============================] - ETA: 0s - loss: 1.0128 - accuracy: 0.6528WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "350/350 [==============================] - 550s 2s/step - loss: 1.0128 - accuracy: 0.6528 - val_loss: 1.5127 - val_accuracy: 0.4844 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "350/350 [==============================] - ETA: 0s - loss: 0.9602 - accuracy: 0.6689WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "350/350 [==============================] - 555s 2s/step - loss: 0.9602 - accuracy: 0.6689 - val_loss: 1.2268 - val_accuracy: 0.5723 - lr: 0.0010\n",
      "Epoch 9/10\n",
      "350/350 [==============================] - ETA: 0s - loss: 0.9257 - accuracy: 0.6849WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "350/350 [==============================] - 644s 2s/step - loss: 0.9257 - accuracy: 0.6849 - val_loss: 1.4501 - val_accuracy: 0.5125 - lr: 0.0010\n",
      "Epoch 10/10\n",
      "350/350 [==============================] - ETA: 0s - loss: 0.8776 - accuracy: 0.6997WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "350/350 [==============================] - 649s 2s/step - loss: 0.8776 - accuracy: 0.6997 - val_loss: 1.2202 - val_accuracy: 0.5908 - lr: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1a3f5c65510>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the Model \n",
    "AlexNet.fit(train_generator.flow(x_train, y_train, batch_size= batch_size),epochs=epochs, steps_per_epoch=x_train.shape[0]//batch_size, validation_data=val_generator.flow(x_val, y_val,batch_size= batch_size),validation_split=250,callbacks=[lrr], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ba4a7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
